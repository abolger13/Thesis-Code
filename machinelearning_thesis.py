# -*- coding: utf-8 -*-
"""MachineLearning_Thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qpFYdrYhDwL7BcMV4vEI7fONfq3UJP5y

# Things still to do in this file
    Stacking of machine learning models to optimise the best one.
    Create a table of the results of the individual models and then the stacking ones
    Try RNN with time series data

# Loading in and little preprocessing
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/Final Year Project/Data/Joined Datasets')

import pandas as pd
ml_df = pd.read_csv("final_ml_ready_dataset.csv")

import pandas as pd

ml_df.head()

null_values = ml_df.isnull().sum()
null_values[null_values > 0]

ml_df = ml_df.dropna(subset=['Visibility (m)'])

ml_df = ml_df.drop(columns=['Departure Delay', 'Flight Duration (Minutes)'], errors='ignore')

ml_df.head()

print(ml_df.dtypes)

ml_df.shape

import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = ml_df.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

"""# Logistic Regression"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# Features & target
X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle NaN values
X_train_scaled = np.nan_to_num(X_train_scaled, nan=np.nanmedian(X_train_scaled))
X_test_scaled = np.nan_to_num(X_test_scaled, nan=np.nanmedian(X_test_scaled))

# Applied SMOTE to training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Grid Search for Hyperparameter Tuning
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2'],
    'solver': ['lbfgs', 'saga'],
    'class_weight': ['balanced']
}

grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train_resampled, y_train_resampled)

# Best model
best_log_reg = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Predict and evaluate
y_pred = best_log_reg.predict(X_test_scaled)
y_prob = best_log_reg.predict_proba(X_test_scaled)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

print(f"\nModel Accuracy (Tuned with SMOTE): {accuracy:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})", color='navy')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression with SMOTE")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Features & target
X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle NaN values
X_train_scaled = np.nan_to_num(X_train_scaled, nan=np.nanmedian(X_train_scaled))
X_test_scaled = np.nan_to_num(X_test_scaled, nan=np.nanmedian(X_test_scaled))

# Grid Search for Hyperparameter Tuning
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2'],
    'solver': ['lbfgs', 'saga'],
    'class_weight': ['balanced']
}

grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# Best model
best_log_reg = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Predict and evaluate
y_pred = best_log_reg.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Model Accuracy (Tuned): {accuracy:.4f}")
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

"""# XGB Boost"""

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import numpy as np

# Split features and target
X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#  SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Hyperparameter grid for tuning
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

# Initialize model
xgb = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# GridSearchCV setup
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring='roc_auc',
    cv=3,
    verbose=1,
    n_jobs=-1
)

# Fit model
grid_search.fit(X_train_resampled, y_train_resampled)
best_model = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Make predictions
y_pred = best_model.predict(X_test_scaled)
y_pred_prob = best_model.predict_proba(X_test_scaled)[:, 1]

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("\nXGBoost Optimised")
print(f"Accuracy: {accuracy:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("XGBoost ROC Curve (SMOTE)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import shap
import numpy as np

# Initialize SHAP TreeExplainer with the best model
explainer = shap.TreeExplainer(best_model)

# SHAP values for the test set
shap_values = explainer.shap_values(X_test_scaled)
X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns)

# global feature importance
shap.summary_plot(shap_values, X_test_df)

# beeswarm plot
shap.plots.beeswarm(shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_test_df))

# dependence plot for a specific feature
shap.dependence_plot("Wind Speed (KT)", shap_values, X_test_df, interaction_index="Humidity (%)")

shap.dependence_plot("hour_of_day", shap_values, X_test_df)
shap.dependence_plot("Flight Duration (Hours)", shap_values, X_test_df)
shap.dependence_plot("Temperature (C)", shap_values, X_test_df)
shap.dependence_plot("Humidity (%)", shap_values, X_test_df)
shap.dependence_plot("Wind Speed (KT)", shap_values, X_test_df)

instance_idx = 5
if isinstance(shap_values, list):
    shap_val = shap_values[1][instance_idx]
    expected_val = explainer.expected_value[1]
else:
    shap_val = shap_values[instance_idx]
    expected_val = explainer.expected_value

# Force plot for local interpretability
shap.initjs()
shap.force_plot(
    expected_val,
    shap_val,
    X_test_df.iloc[instance_idx],
    matplotlib=True
)

import matplotlib.pyplot as plt
import numpy as np

importances = xgb_model.feature_importances_
features = X.columns

# Sorted features by importance
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("XGBoost Feature Importance")
plt.bar(range(len(importances)), importances[indices])
plt.xticks(range(len(importances)), features[indices], rotation=90)
plt.tight_layout()
plt.show()

for i in indices:
    print(f"{features[i]}: {importances[i]:.4f}")

import pandas as pd
import shap
import matplotlib.pyplot as plt


instance_idx = 5

if isinstance(shap_values, list):
    shap_val = shap_values[1][instance_idx]
else:
    shap_val = shap_values[instance_idx]


feature_names = X_test_df.columns
feature_values = X_test_df.iloc[instance_idx]


shap_df = pd.DataFrame({
    'Feature': feature_names,
    'SHAP Value': shap_val,
    'Feature Value': feature_values
})

shap_df['Abs SHAP'] = shap_df['SHAP Value'].abs()
top5_shap = shap_df.sort_values(by='Abs SHAP', ascending=False).head(5)
top5_shap = top5_shap.drop(columns='Abs SHAP')
print(top5_shap)


plt.figure(figsize=(8, 4))
plt.barh(top5_shap['Feature'], top5_shap['SHAP Value'], color='skyblue')
plt.xlabel('SHAP Value')
plt.title('Top 5 Feature Influences on Prediction (Instance {})'.format(instance_idx))
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""# GBM and LightGBM

```
# This is formatted as code
```


"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Gradient Boosting
gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbm.fit(X_train_scaled, y_train)

# Predict
y_pred = gbm.predict(X_test_scaled)
y_prob = gbm.predict_proba(X_test_scaled)[:, 1]

# Plot
print("Gradient Boosting (sklearn) Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

# Gradient Boosting model
gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbm.fit(X_train_smote, y_train_smote)

y_pred = gbm.predict(X_test_scaled)
y_prob = gbm.predict_proba(X_test_scaled)[:, 1]

# Results
print("Gradient Boosting with SMOTE Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_score(y_test, y_prob):.2f})", color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Gradient Boosting ROC Curve (with SMOTE)")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import pandas as pd

# Features and target
X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

# Parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

# Initialize the model
gbm = GradientBoostingClassifier(random_state=42)

# GridSearchCV
grid_search = GridSearchCV(estimator=gbm,
                           param_grid=param_grid,
                           scoring='roc_auc',
                           cv=5,
                           n_jobs=-1,
                           verbose=1)

# grid search
grid_search.fit(X_train_smote, y_train_smote)

# Best model
best_gbm = grid_search.best_estimator_

# Predictions
y_pred = best_gbm.predict(X_test_scaled)
y_prob = best_gbm.predict_proba(X_test_scaled)[:, 1]

# Results
print("Best Hyperparameters:", grid_search.best_params_)
print("Gradient Boosting with SMOTE and Grid Search Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_score(y_test, y_prob):.2f})", color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Gradient Boosting ROC Curve (with SMOTE + Grid Search)")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

pip install lightgbm

from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train LightGBM
lgbm_model = LGBMClassifier()
lgbm_model.fit(X_train, y_train)

# Predictions
y_pred_lgbm = lgbm_model.predict(X_test)

# Evaluation
print("LightGBM Accuracy:", accuracy_score(y_test, y_pred_lgbm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lgbm))
print("Classification Report:\n", classification_report(y_test, y_pred_lgbm))

from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
import matplotlib.pyplot as plt

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# pipeline with SMOTE and LightGBM
pipeline = Pipeline(steps=[
    ('smote', SMOTE(random_state=42)),
    ('lgbm', LGBMClassifier(random_state=42))
])

# hyperparameter grid
param_grid = {
    'lgbm__n_estimators': [100, 200],
    'lgbm__learning_rate': [0.01, 0.1],
    'lgbm__max_depth': [3, 5, 7],
    'lgbm__num_leaves': [15, 31, 50]
}

# Grid search with 5-fold cross-validation
grid_search = GridSearchCV(estimator=pipeline,
                           param_grid=param_grid,
                           scoring='roc_auc',
                           cv=5,
                           n_jobs=-1,
                           verbose=1)

# Fit the model
grid_search.fit(X_train_scaled, y_train)

# Best model
best_model = grid_search.best_estimator_

# Predictions
y_pred = best_model.predict(X_test_scaled)
y_prob = best_model.predict_proba(X_test_scaled)[:, 1]

# Results
print("LightGBM with SMOTE and GridSearchCV Results:")
print("Best Parameters:", grid_search.best_params_)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"ROC AUC Score: {roc_auc_score(y_test, y_prob):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# ROC Curve Plot
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_score(y_test, y_prob):.2f})", color='green')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Optimized LightGBM ROC Curve (with SMOTE)")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

"""# CNN recommend in the paper"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import class_weight
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

#Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#  CNN input
X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Compute class weights
weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(weights))

# CNN model
cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

#Compile
cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train model
cnn_model.fit(
    X_train_cnn,
    y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    class_weight=class_weights,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate model
y_pred_prob_cnn = cnn_model.predict(X_test_cnn)
y_pred_cnn = (y_pred_prob_cnn > 0.5).astype("int32").flatten()

accuracy_cnn = accuracy_score(y_test, y_pred_cnn)
roc_auc_cnn = roc_auc_score(y_test, y_pred_prob_cnn)
f1_macro_cnn = f1_score(y_test, y_pred_cnn, average='macro')
conf_matrix_cnn = confusion_matrix(y_test, y_pred_cnn)
class_report_cnn = classification_report(y_test, y_pred_cnn)

# Output results
print(f"CNN Results:")
print(f"Accuracy     : {accuracy_cnn:.4f}")
print(f"ROC-AUC      : {roc_auc_cnn:.4f}")
print(f"F1 Macro     : {f1_macro_cnn:.4f}")
print("\nConfusion Matrix:\n", conf_matrix_cnn)
print("\nClassification Report:\n", class_report_cnn)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np


X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Reshape for Conv1D
X_train_cnn = X_train_resampled.reshape((X_train_resampled.shape[0], X_train_resampled.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

#Build CNN model
def build_cnn_model(filters=64, kernel_size=3, dense_units=32, dropout_rate=0.3, learning_rate=0.001):
    model = Sequential([
        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
        MaxPooling1D(pool_size=2),
        Dropout(dropout_rate),
        Flatten(),
        Dense(dense_units, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Train model
cnn_model = build_cnn_model()
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

cnn_model.fit(
    X_train_cnn,
    y_train_resampled,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate
y_pred_prob_cnn = cnn_model.predict(X_test_cnn)
y_pred_cnn = (y_pred_prob_cnn > 0.5).astype("int32").flatten()

accuracy_cnn = accuracy_score(y_test, y_pred_cnn)
roc_auc_cnn = roc_auc_score(y_test, y_pred_prob_cnn)
f1_macro_cnn = f1_score(y_test, y_pred_cnn, average='macro')
conf_matrix_cnn = confusion_matrix(y_test, y_pred_cnn)
class_report_cnn = classification_report(y_test, y_pred_cnn)

print(f"📊 CNN with SMOTE Results:")
print(f"Accuracy     : {accuracy_cnn:.4f}")
print(f"ROC-AUC      : {roc_auc_cnn:.4f}")
print(f"F1 Macro     : {f1_macro_cnn:.4f}")
print("\nConfusion Matrix:\n", conf_matrix_cnn)
print("\nClassification Report:\n", class_report_cnn)

"""# MLP"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Standardize
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# MLP model
def build_mlp_model(input_dim, dropout_rate=0.3, learning_rate=0.001):
    model = Sequential([
        Dense(128, activation='relu', input_dim=input_dim),
        Dropout(dropout_rate),
        Dense(64, activation='relu'),
        Dropout(dropout_rate),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
    return model

mlp_model = build_mlp_model(X_train_resampled.shape[1])

# Training
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
mlp_model.fit(
    X_train_resampled, y_train_resampled,
    validation_split=0.1,
    epochs=50,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# Evaluation
y_pred_prob_mlp = mlp_model.predict(X_test_scaled)
y_pred_mlp = (y_pred_prob_mlp > 0.5).astype('int32').flatten()

print("📊 MLP Results:")
print(f"Accuracy     : {accuracy_score(y_test, y_pred_mlp):.4f}")
print(f"F1 Macro     : {f1_score(y_test, y_pred_mlp, average='macro'):.4f}")
print(f"ROC AUC      : {roc_auc_score(y_test, y_pred_prob_mlp):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_mlp))
print("Classification Report:\n", classification_report(y_test, y_pred_mlp))

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score
import pandas as pd
import numpy as np
import itertools
from tqdm import tqdm


X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# hyperparameter grid
param_grid = {
    'hidden_units': [(128, 64), (256, 128, 64)],
    'dropout_rate': [0.2, 0.3],
    'learning_rate': [0.001, 0.0005]
}

# all combinations of hyperparameters
param_combinations = list(itertools.product(*param_grid.values()))
param_names = list(param_grid.keys())

# Storage for results
cv_results = []

# 3-fold cross-validation
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# model builder
def build_mlp(input_dim, hidden_units, dropout_rate, learning_rate):
    model = Sequential()
    model.add(Dense(hidden_units[0], activation='relu', input_dim=input_dim))
    model.add(Dropout(dropout_rate))
    for units in hidden_units[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# grid search
for combo in tqdm(param_combinations, desc="Grid Search"):
    params = dict(zip(param_names, combo))
    fold_scores = []

    for train_idx, val_idx in skf.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_fold)
        X_val_scaled = scaler.transform(X_val_fold)

        # SMOTE
        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train_fold)

        # Build model
        model = build_mlp(
            input_dim=X_train_resampled.shape[1],
            hidden_units=params['hidden_units'],
            dropout_rate=params['dropout_rate'],
            learning_rate=params['learning_rate']
        )

        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)

        model.fit(
            X_train_resampled, y_train_resampled,
            validation_data=(X_val_scaled, y_val_fold),
            epochs=30,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )

        # Predict
        y_val_prob = model.predict(X_val_scaled).flatten()
        y_val_pred = (y_val_prob > 0.5).astype(int)
        f1 = f1_score(y_val_fold, y_val_pred, average='macro')
        fold_scores.append(f1)

    avg_f1 = np.mean(fold_scores)
    cv_results.append({**params, 'avg_f1_macro': avg_f1})

# Create DataFrame and sort by best score
results_df = pd.DataFrame(cv_results).sort_values(by='avg_f1_macro', ascending=False)
best_params = results_df.iloc[0].to_dict()

print("\n Best MLP Hyperparameters:")
print(best_params)

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Standardize
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# final MLP with best hyperparameters
def build_final_mlp(input_dim):
    model = Sequential([
        Dense(256, activation='relu', input_dim=input_dim),
        Dropout(0.2),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Build and train
final_mlp = build_final_mlp(X_train_resampled.shape[1])
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

final_mlp.fit(
    X_train_resampled, y_train_resampled,
    validation_split=0.1,
    epochs=50,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate
y_pred_prob_mlp = final_mlp.predict(X_test_scaled).flatten()
y_pred_mlp = (y_pred_prob_mlp > 0.5).astype(int)

print("📊 Final MLP Evaluation Results:")
print(f"Accuracy     : {accuracy_score(y_test, y_pred_mlp):.4f}")
print(f"F1 Macro     : {f1_score(y_test, y_pred_mlp, average='macro'):.4f}")
print(f"ROC AUC      : {roc_auc_score(y_test, y_pred_prob_mlp):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_mlp))
print("Classification Report:\n", classification_report(y_test, y_pred_mlp))

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# ROC Curve for MLP
fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_pred_prob_mlp)
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)

# ROC Curve
plt.figure(figsize=(8, 5))
plt.plot(fpr_mlp, tpr_mlp, label=f"MLP Neural Network (AUC = {roc_auc_mlp:.2f})", color='red')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - MLP Neural Network")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

"""# Random Forest"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
import shap
import matplotlib.pyplot as plt

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

# Train Random Forest
rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train_smote, y_train_smote)

# Predict
y_pred_rf = rf_model.predict(X_test_scaled)
y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

#Evaluate
accuracy_rf = accuracy_score(y_test, y_pred_rf)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
class_report_rf = classification_report(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_prob_rf)

print(f"Random Forest (with SMOTE) Accuracy: {accuracy_rf:.4f}")
print(f"ROC AUC Score: {roc_auc_rf:.4f}")
print("\nConfusion Matrix:\n", conf_matrix_rf)
print("\nClassification Report:\n", class_report_rf)

# ROC Curve Plot
fpr, tpr, thresholds = roc_curve(y_test, y_prob_rf)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_rf:.2f})", color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest ROC Curve (with SMOTE)")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
import shap
import matplotlib.pyplot as plt
import numpy as np

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

# GridSearchCV for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(random_state=42, n_jobs=-1)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='roc_auc', verbose=1, n_jobs=-1)
grid_search.fit(X_train_smote, y_train_smote)

# Best model
best_rf = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

#  Predict
y_pred_rf = best_rf.predict(X_test_scaled)
y_prob_rf = best_rf.predict_proba(X_test_scaled)[:, 1]

#  Evaluate
accuracy_rf = accuracy_score(y_test, y_pred_rf)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
class_report_rf = classification_report(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_prob_rf)

print(f"\n📊 Random Forest Optimised:")
print(f"Accuracy     : {accuracy_rf:.4f}")
print(f"ROC AUC Score: {roc_auc_rf:.4f}")
print("Confusion Matrix:\n", conf_matrix_rf)
print("Classification Report:\n", class_report_rf)

#  ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob_rf)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_rf:.2f})", color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest ROC Curve (SMOTE + GridSearchCV)")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

import shap
import pandas as pd

#unscaled data here
explainer = shap.TreeExplainer(best_rf)
shap_values = explainer.shap_values(X_test)

# Convert to DataFrame if needed
X_test_df = pd.DataFrame(X_test, columns=X.columns)

# Summary plot
shap.summary_plot(shap_values, X_test_df)

# New API beeswarm plot
shap.plots.beeswarm(
    shap.Explanation(values=shap_values,
                     base_values=explainer.expected_value,
                     data=X_test_df,
                     feature_names=X.columns)
)

# Dependence plot
shap.dependence_plot("Airline", shap_values, X_test_df)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold

rf = RandomForestClassifier(random_state=42)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')

print(f"Cross-Validated Accuracy: {scores.mean():.4f} ± {scores.std():.4f}")

import shap
import pandas as pd

# Convert test data to DataFrame with column names
X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns)

# Sample a smaller portion of test data for SHAP
X_sample = X_test_df.sample(n=500, random_state=42)

#SHAP explainer
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_sample)

#SHAP Summary Plot
shap.summary_plot(shap_values, X_sample)

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

# KNN model (k=5)
knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
knn_model.fit(X_train_scaled, y_train)

# Predictions & Evaluation
y_pred_knn = knn_model.predict(X_test_scaled)

print("KNN Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import matplotlib.pyplot as plt

# SMOTE object
smote = SMOTE(random_state=42)

#pipeline with SMOTE and KNN
pipeline = ImbPipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', smote),
    ('knn', KNeighborsClassifier(n_jobs=-1))
])

# hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],
    'knn__weights': ['uniform', 'distance'],
    'knn__metric': ['euclidean', 'manhattan']
}

# Grid search with 5-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model
best_knn = grid_search.best_estimator_

# Predict on test set
y_pred_knn = best_knn.predict(X_test)
y_proba_knn = best_knn.predict_proba(X_test)[:, 1]

# Evaluation
print("Optimized KNN Results with SMOTE:")
print("Best Parameters:", grid_search.best_params_)
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))

# ROC AUC Score
roc_auc_knn = roc_auc_score(y_test, y_proba_knn)
print(f"ROC AUC Score: {roc_auc_knn:.4f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba_knn)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'KNN (AUC = {roc_auc_knn:.2f})', color='darkblue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('ROC Curve - Optimized KNN with SMOTE')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""# ADA Classifier"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#Define features and target
X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# AdaBoost model
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)
ada_model.fit(X_train_scaled, y_train)

# Predictions & Evaluation
y_pred_ada = ada_model.predict(X_test_scaled)

print("AdaBoost Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_ada):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_ada))
print("Classification Report:\n", classification_report(y_test, y_pred_ada))

from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# Define features and target
X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build pipeline
pipeline = ImbPipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('ada', AdaBoostClassifier(random_state=42))
])

# hyperparameter grid
param_grid = {
    'ada__n_estimators': [50, 100, 200],
    'ada__learning_rate': [0.01, 0.1, 1.0]
}

#Grid Search with Cross-Validation
grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Predict using best model
best_ada = grid_search.best_estimator_
y_pred_ada = best_ada.predict(X_test)
y_proba_ada = best_ada.predict_proba(X_test)[:, 1]  # For ROC AUC

# Evaluation
print("Optimized AdaBoost Results with SMOTE:")
print("Best Parameters:", grid_search.best_params_)
print(f"Accuracy: {accuracy_score(y_test, y_pred_ada):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_ada))
print("Classification Report:\n", classification_report(y_test, y_pred_ada))

# ROC AUC Score
roc_auc_ada = roc_auc_score(y_test, y_proba_ada)
print(f"ROC AUC Score: {roc_auc_ada:.4f}")

# ROC Curve Plot
fpr, tpr, thresholds = roc_curve(y_test, y_proba_ada)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AdaBoost (AUC = {roc_auc_ada:.2f})', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('ROC Curve - Optimized AdaBoost with SMOTE')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""# Naive Bayes

"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train_scaled, y_train)

# Predict on test data
y_pred_nb = nb_model.predict(X_test_scaled)

# Evaluate
print("Naive Bayes Model Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_nb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_nb))

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#  SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

# Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train_smote, y_train_smote)

# Predict on test data
y_pred_nb = nb_model.predict(X_test_scaled)
y_prob_nb = nb_model.predict_proba(X_test_scaled)[:, 1]

# Evaluate
print("Naive Bayes with SMOTE Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_nb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_nb))
print(f"ROC AUC Score: {roc_auc_score(y_test, y_prob_nb):.4f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob_nb)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_score(y_test, y_prob_nb):.2f})", color='orange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Naive Bayes ROC Curve (with SMOTE)")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

"""# SVM"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

#Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train SVM model
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)
svm_model.fit(X_train, y_train)

# Predict and evaluate
y_pred_svm = svm_model.predict(X_test)
y_proba_svm = svm_model.predict_proba(X_test)[:, 1]  # Probability estimates for ROC AUC

# Evaluation
print("SVM Model Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

# ROC AUC Score
roc_auc_svm = roc_auc_score(y_test, y_proba_svm)
print(f"ROC AUC Score: {roc_auc_svm:.4f}")

# ROC Curve Plot
fpr, tpr, thresholds = roc_curve(y_test, y_proba_svm)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'SVM (AUC = {roc_auc_svm:.2f})', color='purple')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('ROC Curve - SVM')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
import matplotlib.pyplot as plt

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# pipeline with SMOTE and SVC
pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('svc', SVC(probability=True, random_state=42))
])

# hyperparameter grid
param_grid = {
    'svc__C': [0.1, 1, 10],
    'svc__gamma': ['scale', 0.01, 0.1, 1],
    'svc__kernel': ['rbf']  # You can also include 'linear' or 'poly' if you want
}

# Grid search
grid_search = GridSearchCV(pipeline,
                           param_grid=param_grid,
                           scoring='roc_auc',
                           cv=3,
                           n_jobs=-1,
                           verbose=1)

# Fit the model
grid_search.fit(X_train_scaled, y_train)

# Best model
best_svc_model = grid_search.best_estimator_

# Predictions
y_pred = best_svc_model.predict(X_test_scaled)
y_prob = best_svc_model.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("SVM with SMOTE and GridSearchCV Results:")
print("Best Parameters:", grid_search.best_params_)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print(f"ROC AUC Score: {roc_auc_score(y_test, y_prob):.4f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_score(y_test, y_prob):.2f})", color='purple')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("SVM ROC Curve (with SMOTE + Grid Search)")
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

"""#Stacking three models attempt"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# Preprocessing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# base learners
base_learners = [
    ('logreg', LogisticRegression(max_iter=1000)),
    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),
    ('mlp', MLPClassifier(hidden_layer_sizes=(128, 64), alpha=0.001, learning_rate_init=0.001,
                          solver='adam', early_stopping=True, max_iter=1000, random_state=42))
]

# Meta-model
meta_model = LogisticRegression()

# StackingClassifier
stacked_model = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_model,
    cv=5,
    passthrough=False,
    n_jobs=1
)


# Fit and evaluate
stacked_model.fit(X_train, y_train)
y_pred_stack = stacked_model.predict(X_test)

print("Stacked Ensemble Accuracy:", accuracy_score(y_test, y_pred_stack))
print("Classification Report:\n", classification_report(y_test, y_pred_stack))

from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Define base models
rf = RandomForestClassifier(n_estimators=100, random_state=42)
lgbm = LGBMClassifier(random_state=42)
nn = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=200, random_state=42)

# Stack them with Logistic Regression as meta-learner
stacked_model = StackingClassifier(
    estimators=[
        ('rf', rf),
        ('lgbm', lgbm),
        ('nn', nn)
    ],
    final_estimator=LogisticRegression(),
    cv=5,
    passthrough=False,
    n_jobs=-1
)

#Fit the model
stacked_model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = stacked_model.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"📦 Stacking Model Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Predict probabilities for the positive class
y_pred_proba = stacked_model.predict_proba(X_test_scaled)[:, 1]

# ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('📈 ROC Curve - Stacking Model')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

pip install shap

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], X_test_scaled[0], feature_names=X.columns)

pip install lime

from lime.lime_tabular import LimeTabularExplainer

explainer_lime = LimeTabularExplainer(
    training_data=X_train_scaled,
    feature_names=X.columns,
    class_names=['On-Time', 'Delayed'],
    mode='classification'
)

# Explain a single prediction
i = 0  # row index
exp = explainer_lime.explain_instance(X_test_scaled[i], stacked_model.predict_proba, num_features=10)
exp.show_in_notebook(show_table=True)

"""# Optimised stacking models based on the results"""

from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

X = ml_df.drop(columns=['Delayed'])
y = ml_df['Delayed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SMOTE and scaling pipeline
preprocess = ImbPipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42))
])

X_train_processed, y_train_processed = preprocess.fit_resample(X_train, y_train)
X_test_scaled = StandardScaler().fit(X_train).transform(X_test)

# Base learners
base_learners = [
    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),
    ('rf', RandomForestClassifier(random_state=42)),
    ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42))
]

# Meta learner
meta_learner = LogisticRegression()

# Stacking Classifier
stacking_model = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5,
    n_jobs=-1
)

# stacking model
stacking_model.fit(X_train_processed, y_train_processed)

# Predict and evaluate
y_pred_stack = stacking_model.predict(X_test_scaled)
y_proba_stack = stacking_model.predict_proba(X_test_scaled)[:, 1]

print("Stacking Classifier Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred_stack))
print(f"ROC AUC Score: {roc_auc_score(y_test, y_proba_stack):.4f}")

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_stack)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'Stacked Model (AUC = {roc_auc_score(y_test, y_proba_stack):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve - Stacked Model')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

import shap
import numpy as np
import pandas as pd

# Get the base models
base_models = [model for _, model in stacking_model.named_estimators_.items()]

# Generate prediction probabilities from each base model
meta_input_test = np.column_stack([
    model.predict_proba(X_test_scaled)[:, 1] for model in base_models
])

#name the columns for interpretability
meta_input_df = pd.DataFrame(meta_input_test, columns=[name for name, _ in base_learners])

# Get the trained meta-learner
meta_model = stacking_model.final_estimator_

# SHAP analysis on meta-learner
explainer = shap.Explainer(meta_model, meta_input_df)
shap_values = explainer(meta_input_df)

# Visualise global feature importance
shap.plots.beeswarm(shap_values)

# summary bar plot
shap.plots.bar(shap_values)

# dependence plot to see how one base model's prediction affects output
shap.dependence_plot("rf", shap_values.values, meta_input_df)

# one instance to explain
instance_idx = 5




#waterfall plot for that instance
shap.plots.waterfall(shap_values[instance_idx])

"""# Presenting Data in Tables

Questions to ask tomorrow - Should i run the cluster analysis to compare to raw weather data

Should i do time series analysis
"""

import pandas as pd

# model results table
data = {
    "Model": [
        "Logistic Regression", "XGBoost", "Gradient Boosting", "Light Gradient Boosting",
        "Convolutional Neural Networks", "SVM", "MLP (Neural Networks)", "Random Forest",
        "KNN", "Ada Classifier", "Naive Bayes", "Stacking 1 (XGB + RF + MLP + LG)"
    ],
    "Model Accuracy": [0.64, 0.83, 0.80, 0.81, 0.69, 0.76, 0.72, 0.85, 0.74, 0.69, 0.61, 0.86],
    "Precision": [0.70, 0.83, 0.80, 0.80, 0.73, 0.75, 0.76, 0.85, 0.76, 0.72, 0.69, 0.85],
    "Recall": [0.64, 0.83, 0.80, 0.81, 0.69, 0.76, 0.72, 0.85, 0.74, 0.69, 0.61, 0.86],
    "F1 Score": [0.65, 0.83, 0.80, 0.80, 0.70, 0.72, 0.73, 0.85, 0.75, 0.70, 0.63, 0.85],
    "AUC SCORE": [0.68, 0.87, 0.84, 0.85, 0.75, 0.74, 0.79, 0.91, 0.77, 0.73, 0.66, 0.91]
}

results_df = pd.DataFrame(data)
ranked_df = results_df.sort_values(by=["Model Accuracy", "AUC SCORE"], ascending=False).reset_index(drop=True)

# ranked table
ranked_df.style.set_caption("Ranked Classification Models by Accuracy and AUC Score").format(precision=2)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


y_prob_logreg = y_prob
y_prob_lgbm = y_prob
y_prob_gb = y_prob
y_prob_xgb = y_pred_prob

# model names and their ROC values
model_data = {
    "Logistic Regression": roc_curve(y_test, y_prob_logreg),
    "Random Forest": roc_curve(y_test, y_prob_rf),
    "Gradient Boosting": roc_curve(y_test, y_prob_gb),
    "XGBoost": roc_curve(y_test, y_prob_xgb),
    "LightGBM": roc_curve(y_test, y_prob_lgbm),
    "AdaBoost": roc_curve(y_test, y_proba_ada),
    "Naive Bayes": roc_curve(y_test, y_prob_nb),
    "KNN": roc_curve(y_test, y_proba_knn),
    "SVM": roc_curve(y_test, y_proba_svm),
    "MLP Neural Net": roc_curve(y_test, y_pred_prob_mlp),
    "Stacking Ensemble": roc_curve(y_test, y_proba_stack),
}

# AUCs and packed all info into a list
model_metrics = []
for name, (fpr, tpr, _) in model_data.items():
    model_metrics.append({
        "name": name,
        "fpr": fpr,
        "tpr": tpr,
        "auc": auc(fpr, tpr)
    })

# Sort by AUC descending
model_metrics = sorted(model_metrics, key=lambda x: x["auc"], reverse=True)

# Plot
plt.figure(figsize=(15, 9))
colors = [
    'black', 'red', 'darkorange', 'navy', 'blue', 'green',
    'teal', 'orange', 'purple', 'darkblue', 'mediumorchid'
]

for i, model in enumerate(model_metrics):
    linestyle = '-.' if model["name"] == "SVM" else '-'
    lw = 2.5 if model["name"] == "Stacking Ensemble" else 2
    plt.plot(
        model["fpr"],
        model["tpr"],
        label=f'{model["name"]} (AUC = {model["auc"]:.2f})',
        color=colors[i % len(colors)],
        linestyle=linestyle,
        linewidth=lw
    )

plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison: All Models (Ordered by AUC)")
plt.legend(loc='lower right', fontsize='small', frameon=True)
plt.grid(True)
plt.tight_layout()
plt.show()