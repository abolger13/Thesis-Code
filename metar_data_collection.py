# -*- coding: utf-8 -*-
"""METAR_Data_Collection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iiin8i8_QFn4gZtn2uyoJy0xs6NzC3gw
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/Final Year Project/Data')
!pwd

"""# Metar weather data collection

from https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=REV&nil=SI&fmt=html&ano=2025&mes=02&day=09&hora=14&anof=2025&mesf=02&dayf=10&horaf=14&minf=59&send=send using data scraping
"""

pip install metar

import requests
from bs4 import BeautifulSoup

# URL for Ogimet METAR data
url = "https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=REV&nil=SI&fmt=html&ano=2024&mes=02&day=01&hora=01&anof=2024&mesf=02&dayf=29&horaf=23&minf=59&send=send"
# Request page content
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Extract METAR strings from <pre> tags
metar_strings = [pre.get_text().strip() for pre in soup.find_all('pre')]

# Display extracted METAR data
for metar in metar_strings:
    print(metar)

"""THIS CODE BELOW WORKS FOR INDIVIDUAL DATA SCRAPING FROM THE LIST OF LINKS THAT I HAVE STORED ON LAPTOP"""

import requests
from bs4 import BeautifulSoup

# URL for 2023 METAR data
url = "https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=12&day=01&hora=00&anof=2024&mesf=12&dayf=31&horaf=23&minf=59&send=send"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# Make the request with headers
response = requests.get(url, headers=headers)

# Parse the page content
soup = BeautifulSoup(response.text, 'html.parser')

# Extract METAR strings from <pre> tags
metar_strings = [td.get_text().strip() for td in soup.find_all('pre')]

# Verify scraped METARs
print(f"Total METARs found: {len(metar_strings)}")
print("First few METARs:", metar_strings[:5])

import numpy as np

def calculate_humidity(temp, dew_point):
    if temp == "N/A" or dew_point == "N/A":
        return "N/A"
    return round(100 * (np.exp((17.625 * dew_point) / (243.04 + dew_point)) / np.exp((17.625 * temp) / (243.04 + temp))), 2)

import pandas as pd
from metar import Metar  # Ensure you have `metar` installed via pip

data = []

for metar_string in metar_strings:
    try:
        metar = Metar.Metar(metar_string)  # Parse METAR data
        data.append({
            "Station": metar.station_id,
            "Observation Time": metar.time.strftime('%Y-%m-%d %H:%M:%S') if metar.time else "N/A",
            "Wind Speed (KT)": metar.wind_speed.value("KT") if metar.wind_speed else "N/A",
            "Visibility (m)": metar.vis.value("m") if metar.vis else "N/A",
            "Temperature (C)": metar.temp.value("C") if metar.temp else "N/A",
            "Dew Point (C)": metar.dewpt.value("C") if metar.dewpt else "N/A",
            "Pressure (hPa)": metar.press.value("hPa") if metar.press else "N/A",
            "Humidity (%)": calculate_humidity(metar.temp.value("C"), metar.dewpt.value("C")),        })
    except Exception as e:
        print("Error parsing METAR:", e)

# Convert structured METAR data to DataFrame
df = pd.DataFrame(data)
df.to_csv("December_Metar.csv", index=False)
# Display first few rows to verify
print(df.head())

import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from metar import Metar  # Ensure you have the metar package installed

# Function to calculate relative humidity
def calculate_humidity(temp, dew_point):
    if temp == "N/A" or dew_point == "N/A":
        return "N/A"
    return round(100 * (np.exp((17.625 * dew_point) / (243.04 + dew_point)) /
                        np.exp((17.625 * temp) / (243.04 + temp))), 2)

# List of URLs to scrape METAR data from
urls = ["https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=SA&ord=DIR&nil=SI&fmt=html&ano=2024&mes=01&day=01&hora=00&anof=2024&mesf=01&dayf=31&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=SA&ord=DIR&nil=SI&fmt=html&ano=2024&mes=02&day=01&hora=00&anof=2024&mesf=02&dayf=29&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=REV&nil=SI&fmt=html&ano=2024&mes=03&day=01&hora=00&anof=2024&mesf=03&dayf=31&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=04&day=01&hora=00&anof=2024&mesf=04&dayf=30&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=05&day=01&hora=00&anof=2024&mesf=05&dayf=31&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=06&day=01&hora=00&anof=2024&mesf=06&dayf=30&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=07&day=01&hora=00&anof=2024&mesf=07&dayf=31&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=08&day=01&hora=00&anof=2024&mesf=08&dayf=31&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=09&day=01&hora=00&anof=2024&mesf=09&dayf=30&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=10&day=01&hora=00&anof=2024&mesf=10&dayf=31&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=11&day=01&hora=00&anof=2024&mesf=11&dayf=30&horaf=23&minf=59&send=send",
"https://www.ogimet.com/display_metars2.php?lang=en&lugar=EIDW&tipo=ALL&ord=DIR&nil=SI&fmt=html&ano=2024&mes=12&day=01&hora=00&anof=2024&mesf=12&dayf=31&horaf=23&minf=59&send=send"
]

data = []  # Store parsed METAR data

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extract METAR strings from <pre> tags
    metar_strings = [pre.get_text().strip() for pre in soup.find_all('pre')]

    for metar_string in metar_strings:
        try:
            metar = Metar.Metar(metar_string)  # Parse METAR data
            temp_c = metar.temp.value("C") if metar.temp else "N/A"
            dew_c = metar.dewpt.value("C") if metar.dewpt else "N/A"

            data.append({
                "Station": metar.station_id,
                "Observation Time": metar.time.strftime('%Y-%m-%d %H:%M:%S') if metar.time else "N/A",
                "Wind Speed (KT)": metar.wind_speed.value("KT") if metar.wind_speed else "N/A",
                "Visibility (m)": metar.vis.value("m") if metar.vis else "N/A",
                "Temperature (C)": temp_c,
                "Dew Point (C)": dew_c,
                "Humidity (%)": calculate_humidity(temp_c, dew_c),
                "Pressure (hPa)": metar.press.value("hPa") if metar.press else "N/A",
            })
        except Exception as e:
            print("Error parsing METAR:", e)

# Convert to DataFrame and append to CSV
df = pd.DataFrame(data)
df.to_csv("metar_data.csv", mode='a', index=False, header=not pd.io.common.file_exists("metar_data.csv"))

print("CSV file updated successfully with new METAR data!")

"""JOINING THE 12 DATASETS TOGETHER AND ORDERING THEM BY DATE AND TIME"""

import pandas as pd
import os

# Folder containing CSV files
folder_path = "/content/drive/MyDrive/safety"  # Update with the actual path

# Get all CSV files in the folder
csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

# Read and concatenate all CSVs
df_list = [pd.read_csv(os.path.join(folder_path, file)) for file in csv_files]
merged_df = pd.concat(df_list, ignore_index=True)

# Save the merged file
merged_df.to_csv("/content/drive/MyDrive/safety/merged_output.csv", index=False)


print("All files have been merged successfully!")

import pandas as pd

# Load the merged CSV file
df = pd.read_csv("merged_output.csv")

# Convert the 'Observation Time' column to datetime format
df['Observation Time'] = pd.to_datetime(df['Observation Time'], dayfirst=True, errors='coerce')

# Sort by date and time
df = df.sort_values(by='Observation Time')

# Save the sorted file
df.to_csv("/content/drive/MyDrive/safety/sorted_output.csv", index=False)

print("File sorted successfully!")

"""# Rest API interaction for flight data collection

Data collected from https://aviationstack.com/
"""

pip install requests

import requests
import json
import csv

# API details
base_url = "https://api.aviationstack.com/v1/"
endpoint = "flights"
access_key = "90356201bafcece5d0a8f8e7197a7e5e"  # Replace with your actual access key

# Query parameters to filter flights departing from Dublin Airport (DUB)
params = {
    'access_key': access_key,
    'dep_iata': 'DUB',  # Departures from Dublin
    'limit': 5  # Number of records to fetch
}

# Send request to the API
response = requests.get(base_url + endpoint, params=params)

# Check response
if response.status_code == 200:
    data = response.json()

    # Define CSV file name
    csv_filename = "flights_data.csv"

    # Define CSV headers
    headers = [
        "Flight Number", "Airline", "Aircraft Model", "Registration",
        "Departure Airport", "Scheduled Departure", "Estimated Departure",
        "Actual Departure", "Departure Terminal", "Departure Gate", "Departure Delay",
        "Arrival Airport", "Scheduled Arrival", "Estimated Arrival",
        "Actual Arrival", "Arrival Terminal", "Arrival Gate", "Arrival Delay",
        "Flight Status"
    ]

    # Open CSV file and write data
    with open(csv_filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(headers)  # Write header row

        for flight in data.get('data', []):
            def safe_get(dictionary, key, default="N/A"):
                return dictionary.get(key, default) if dictionary else default

            row = [
                safe_get(flight.get('flight'), 'iata'),
                safe_get(flight.get('airline'), 'name'),
                safe_get(flight.get('aircraft'), 'model'),
                safe_get(flight.get('aircraft'), 'registration'),

                safe_get(flight.get('departure'), 'airport'),
                safe_get(flight.get('departure'), 'scheduled'),
                safe_get(flight.get('departure'), 'estimated'),
                safe_get(flight.get('departure'), 'actual'),
                safe_get(flight.get('departure'), 'terminal'),
                safe_get(flight.get('departure'), 'gate'),
                safe_get(flight.get('departure'), 'delay'),

                safe_get(flight.get('arrival'), 'airport'),
                safe_get(flight.get('arrival'), 'scheduled'),
                safe_get(flight.get('arrival'), 'estimated'),
                safe_get(flight.get('arrival'), 'actual'),
                safe_get(flight.get('arrival'), 'terminal'),
                safe_get(flight.get('arrival'), 'gate'),
                safe_get(flight.get('arrival'), 'delay'),

                safe_get(flight, 'flight_status')
            ]

            writer.writerow(row)  # Write flight data row

    print(f"Data successfully saved to {csv_filename}")

else:
    print(f"Error: {response.status_code}, {response.text}")